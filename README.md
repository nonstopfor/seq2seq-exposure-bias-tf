# Sequence Generation Algorithms Tackling Exposure Bias #

Despite the computational simplicity and efficiency, maximum likelihood training of sequence generation models (e.g., RNNs) suffers from the exposure bias [(Ranzato et al., 2015)](https://arxiv.org/pdf/1511.06732.pdf). That is, the model is trained to predict the next token given the previous ground-truth tokens; while at test time, since the resulting model does not have access to the ground truth, tokens generated by the model itself are instead used to make the next prediction. This discrepancy between training and test leads to the issue that mistakes in prediction can quickly accumulate.

This example provide implementations of some classic and advanced training algorithms that tackles the exposure bias. The base model is an attentional seq2seq.

* **Maximum Likelihood (MLE)**: attentional seq2seq model with maximum likelihood training.
* **Reward Augmented Maximum Likelihood (RAML)**: Described in [(Norouzi et al., 2016)](https://arxiv.org/pdf/1609.00150.pdf) and we use the sampling approach (n-gram replacement) by [(Ma et al., 2017)](https://arxiv.org/abs/1705.07136).
* **Scheduled Sampling**: Described in [(Bengio et al., 2015)](https://arxiv.org/abs/1506.03099)

## Motivation

This repository is adapted from [texar](https://github.com/asyml/texar/tree/master/examples/seq2seq_exposure_bias). The main motivation is to use [cotk](https://github.com/thu-coai/cotk) for data loading and model evaluation, which is more convenient. Thus the model implementation is the same as [texar](https://github.com/asyml/texar/tree/master/examples/seq2seq_exposure_bias), but the data loading and model evaluation parts use [cotk](https://github.com/thu-coai/cotk).

## Usage ##

### Dataset ###

Two example datasets are provided:

  * iwslt14: The benchmark [IWSLT2014](https://sites.google.com/site/iwsltevaluation2014/home) (de-en) machine translation dataset, following [(Ranzato et al., 2015)](https://arxiv.org/pdf/1511.06732.pdf) for data pre-processing.
  * gigaword: The benchmark [GIGAWORD](https://catalog.ldc.upenn.edu/LDC2003T05) text summarization dataset. we sampled 200K out of the 3.8M pre-processed training examples provided by [(Rush et al., 2015)](https://www.aclweb.org/anthology/D/D15/D15-1044.pdf) for the sake of training efficiency. We used the refined validation and test sets provided by [(Zhou et al., 2017)](https://arxiv.org/pdf/1704.07073.pdf).

Download the data with the following commands:

```bash
python utils/prepare_data.py --data iwslt14
python utils/prepare_data.py --data giga
```

### Train the models ###

***Important***: To adapt cotk, we need to further process the downloaded dataset. Here are the steps.

1. Unzip "iwslt14.zip" and put the files in data/iwslt14.

2. Unzip "giga.zip" and put the files in data/giga.

3. run the following code to process the dataset (The generated "train.txt", "dev.txt" ,"test.txt" are what we need) :

   ```bash
   cd data/iwslt14
   python process_iwslt14.py
   cd ../giga
   python process_giga.py
   ```

4. Copy the "test.txt" and "dev.txt" from data/iwslt14 to data/iwslt14_raml.

5. Copy the "test.txt" and "dev.txt" from data/giga to data/giga_raml.

6. run the following code to process the dataset (The generated "train.txt" is what we need) :

   ```bash
   cd data/iwslt14_raml
   python format_train.py
   cd ../giga_raml
   python format_train.py
   ```



#### Baseline Attentional Seq2seq

For iwslt14:

```bash
python seq2seq_main.py \
    --config_model configs.config_model \
    --config_data configs.config_iwslt14
```

For giga:

```bash
python seq2seq_main.py \
    --config_model configs.config_model \
    --config_data configs.config_iwslt14
```

Here:

  * `--config_model` specifies the model config. Note not to include the `.py` suffix.
  * `--config_data` specifies the data config.

[configs.config_model.py](./configs/config_model.py) specifies a single-layer seq2seq model with Luong attention and bi-directional RNN encoder. Hyperparameters taking default values can be omitted from the config file. 

#### Reward Augmented Maximum Likelihood (RAML)

For iwslt14:

```bash
python raml_main.py \
    --config_model configs.config_model \
    --config_data configs.config_iwslt14 \
    --n_samples 10 \
    --tau 0.4
```
For giga:

```bash
python raml_main.py \
    --config_model configs.config_model \
    --config_data configs.config_giga \
    --n_samples 10 \
    --tau 0.4
```

Here:

  * `--n_samples` specifies number of augmented samples for every target sentence.
  * `--tau` specifies the temperature of the exponentiated payoff distribution in RAML.

In the downloaded datasets, we have provided example files for `--raml_file`, which including augmented samples for ```iwslt14``` and ```gigaword``` respectively. We also provide scripts for generating augmented samples by yourself. Please refer to [utils/raml_samples_generation](utils/raml_samples_generation).

#### Scheduled Sampling

For iwslt14:

```bash
python scheduled_sampling_main.py \
    --config_model configs.config_model \
    --config_data configs.config_iwslt14 \
    --decay_factor 500.
```
For giga:

```bash
python scheduled_sampling_main.py \
    --config_model configs.config_model \
    --config_data configs.config_giga \
    --decay_factor 500.
```

Here:

  * `--decay_factor` specifies the hyperparameter controling the speed of increasing the probability of sampling from model.

#### Using run.py

Besides the above commands to train the models, you could also use run.py to train each model. Here is the usage.

- For Baseline Attentional Seq2seq

	```
	python run.py --script seq2seq_main
	```
	
	Other parameters are the same as the example in [Baseline Attentional Seq2seq](#baseline-attentional-seq2seq)
	
- For Reward Augmented Maximum Likelihood (RAML)

  ```
  python run.py --script raml_main
  ```

  Other parameters are the same as the example in [Reward Augmented Maximum Likelihood (RAML)](#reward-augmented-maximum-likelihood-raml)

- For Scheduled Sampling

  ```
  python run.py --script scheduled_sampling_main
  ```

  Other parameters are the same as the example in [Scheduled Sampling](#scheduled-sampling)



## Load checkpoints

For all models, if you add "--load True" when running the code, the checkpoint will be loaded. By default, the checkpoint won't be loaded.

For Baseline Attentional Seq2seq, the checkpoint dir is "checkpoints/seq2seq_base/{dataset_name}/".

For Reward Augmented Maximum Likelihood (RAML), the checkpoint dir is "checkpoints/raml/{dataset_name}/".

For Scheduled Sampling, the checkpoint dir is "checkpoints/scheduled_sampling/{dataset_name}/".

**Here the {dataset_name} = giga or iwslt14** . And the dataset_name will be automatically inferred from the  

"config_data" value.

## Results ##

### Machine Translation
| Model      | BLEU Score   | Hyperparameters |
| -----------| -------| -----------|
| MLE   | 0.3021   | num_units=256, beam_width=5, decoder_layers=1, dropout=0.2, learning_rate=0.001, batch_size=64,epoch=10 |
| Scheduled Sampling | 0.3045     | num_units=256, beam_width=5, decoder_layers=1, dropout=0.2, learning_rate=0.001, batch_size=64,epoch=10 |
| RAML | 0.3015    | num_units=256, beam_width=5, decoder_layers=1, dropout=0.2, learning_rate=0.001,batch_size=8，epoch=10 |

### Text Summarization
| Model      | BLEU Score | Hyperparameters |
| -----------|-------| -----------|
| MLE        | 0.1236 | num_units=256, beam_width=5, decoder_layers=1, dropout=0.2, learning_rate=0.001, batch_size=64,epoch=10 |
| Scheduled Sampling   | 0.1234    | num_units=256, beam_width=5, decoder_layers=1, dropout=0.2, learning_rate=0.001, batch_size=64,epoch=10 |
| RAML | 0.1256 | num_units=256, beam_width=5, decoder_layers=1, dropout=0.2, learning_rate=0.001,batch_size=8，epoch=10 |

 ## Acknowledgement

This repository is adapted from texar. 

### License

[Apache License 2.0](LICENSE)